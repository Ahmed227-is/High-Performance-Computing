{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74237e26",
   "metadata": {},
   "source": [
    "# MPI4PY – Fonctions de communication (Notebook pédagogique)\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Préambule\n",
    "\n",
    "Ce notebook regroupe **toutes les fonctions de communication MPI** vues précédemment, avec :\n",
    "\n",
    "* le **prototype exact** en mpi4py,\n",
    "* l’**explication complète des arguments**,\n",
    "* la distinction claire entre **objets Python (sérialisation)** et **buffers mémoire (HPC)**.\n",
    "\n",
    "On suppose l’import suivant dans toutes les cellules :\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Communication point à point – objets Python (sérialisation)\n",
    "\n",
    "### 1.1 `comm.send(obj, dest, tag=0)`\n",
    "\n",
    "Envoie un **objet Python sérialisé**.\n",
    "\n",
    "* `obj` : objet Python sérialisable (int, list, dict, tuple, etc.)\n",
    "* `dest` : rang du processus destinataire\n",
    "* `tag` : identifiant du message\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    comm.send([1, 2, 3], dest=1, tag=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 `comm.recv(source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG, status=None)`\n",
    "\n",
    "Reçoit un **objet Python sérialisé**.\n",
    "\n",
    "* `source` : rang attendu ou `MPI.ANY_SOURCE`\n",
    "* `tag` : tag attendu ou `MPI.ANY_TAG`\n",
    "* `status` : objet `MPI.Status()` optionnel\n",
    "\n",
    "```python\n",
    "if rank == 1:\n",
    "    data = comm.recv(source=0, tag=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Non bloquant (objets)\n",
    "\n",
    "```python\n",
    "req = comm.isend(obj, dest, tag)\n",
    "req = comm.irecv(source, tag)\n",
    "req.wait()\n",
    "```\n",
    "\n",
    "Les fonctions `isend / irecv` retournent un objet `MPI.Request`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Communication point à point – buffers mémoire (HPC)\n",
    "\n",
    "### 2.1 `comm.Send(sendbuf, dest, tag=0)`\n",
    "\n",
    "Envoi **sans sérialisation**.\n",
    "\n",
    "* `sendbuf` : tableau NumPy contigu ou `[array, MPI.TYPE]`\n",
    "* `dest` : rang destinataire\n",
    "* `tag` : tag\n",
    "\n",
    "```python\n",
    "A = np.array([1, 2, 3], dtype=np.float64)\n",
    "comm.Send([A, MPI.DOUBLE], dest=1, tag=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 `comm.Recv(recvbuf, source, tag=0, status=None)`\n",
    "\n",
    "Réception **buffer**.\n",
    "\n",
    "* `recvbuf` : tableau NumPy déjà alloué\n",
    "\n",
    "```python\n",
    "B = np.empty(3, dtype=np.float64)\n",
    "comm.Recv([B, MPI.DOUBLE], source=0, tag=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Non bloquant (buffers)\n",
    "\n",
    "```python\n",
    "req = comm.Isend([A, MPI.DOUBLE], dest=1)\n",
    "req = comm.Irecv([B, MPI.DOUBLE], source=0)\n",
    "req.wait()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Synchronisation\n",
    "\n",
    "### 3.1 `comm.Barrier()`\n",
    "\n",
    "Tous les processus attendent.\n",
    "\n",
    "```python\n",
    "comm.Barrier()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Communication collective – objets Python (sérialisation)\n",
    "\n",
    "### 4.1 `comm.bcast(obj, root=0)`\n",
    "\n",
    "* `obj` : donné sur `root`, `None` ailleurs\n",
    "* `root` : rang source\n",
    "\n",
    "```python\n",
    "data = comm.bcast(data if rank == 0 else None, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 `comm.scatter(sendobj, root=0)`\n",
    "\n",
    "* `sendobj` : liste de taille `size` sur `root`\n",
    "* retour : un élément par processus\n",
    "\n",
    "```python\n",
    "x = comm.scatter(data if rank == 0 else None, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 `comm.gather(sendobj, root=0)`\n",
    "\n",
    "* `sendobj` : objet local\n",
    "* retour : liste sur `root`, `None` ailleurs\n",
    "\n",
    "```python\n",
    "res = comm.gather(rank, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 `comm.allgather(sendobj)`\n",
    "\n",
    "* retour : liste sur tous les processus\n",
    "\n",
    "```python\n",
    "vals = comm.allgather(rank)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 `comm.reduce(sendobj, op=MPI.SUM, root=0)`\n",
    "\n",
    "Réduction **Python** (sérialisation).\n",
    "\n",
    "```python\n",
    "total = comm.reduce(rank, op=MPI.SUM, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.6 `comm.allreduce(sendobj, op=MPI.SUM)`\n",
    "\n",
    "Réduction + diffusion (objets).\n",
    "\n",
    "```python\n",
    "total = comm.allreduce(rank, op=MPI.SUM)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.7 `comm.alltoall(sendobj)`\n",
    "\n",
    "Tous vers tous (objets).\n",
    "\n",
    "```python\n",
    "send = [rank]*size\n",
    "recv = comm.alltoall(send)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Communication collective – buffers mémoire (HPC)\n",
    "\n",
    "### 5.1 `comm.Scatter(sendbuf, recvbuf, root=0)`\n",
    "\n",
    "Tailles fixes.\n",
    "\n",
    "* `sendbuf` : tableau global (root)\n",
    "* `recvbuf` : tableau local (tous)\n",
    "\n",
    "```python\n",
    "comm.Scatter([A, MPI.DOUBLE], [local, MPI.DOUBLE], root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 `comm.Gather(sendbuf, recvbuf, root=0)`\n",
    "\n",
    "Tailles fixes.\n",
    "\n",
    "```python\n",
    "comm.Gather([local, MPI.DOUBLE], [A_out, MPI.DOUBLE], root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 `comm.Reduce(sendbuf, recvbuf, op=MPI.SUM, root=0)`\n",
    "\n",
    "Vraie réduction MPI.\n",
    "\n",
    "```python\n",
    "comm.Reduce([local, MPI.DOUBLE], [global_sum, MPI.DOUBLE], op=MPI.SUM, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 `comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)`\n",
    "\n",
    "Résultat chez tous.\n",
    "\n",
    "```python\n",
    "comm.Allreduce([local, MPI.DOUBLE], [global_sum, MPI.DOUBLE], op=MPI.SUM)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 `comm.Alltoall(sendbuf, recvbuf)`\n",
    "\n",
    "Tous vers tous (buffers, tailles fixes).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Variantes à tailles variables (buffers uniquement)\n",
    "\n",
    "### 6.1 `comm.Scatterv(sendbuf, recvbuf, root=0)`\n",
    "\n",
    "* `sendbuf` (root) : `[A, counts, displs, MPI.TYPE]`\n",
    "* `recvbuf` : `[local, MPI.TYPE]`\n",
    "\n",
    "```python\n",
    "comm.Scatterv([A, counts, displs, MPI.DOUBLE], [local, MPI.DOUBLE], root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 `comm.Gatherv(sendbuf, recvbuf, root=0)`\n",
    "\n",
    "* `sendbuf` : `[local, MPI.TYPE]`\n",
    "* `recvbuf` (root) : `[A_out, counts, displs, MPI.TYPE]`\n",
    "\n",
    "```python\n",
    "comm.Gatherv([local, MPI.DOUBLE], [A_out, counts, displs, MPI.DOUBLE], root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Règle d’or HPC\n",
    "\n",
    "* Objets Python → sérialisation → lent\n",
    "* Buffers NumPy → accès mémoire direct → performant\n",
    "\n",
    "En calcul haute performance : **toujours préférer les versions buffer**.\n",
    "\n",
    "---\n",
    "\n",
    "Fin du notebook.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
