{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSON : Programmation Haute Performance\n",
    "\n",
    "## Partie 2 : les communications collectives\n",
    "\n",
    "Les fonctions de communications collectives sont les fonctions qui permettent de réaliser un ensemble de communications (envoi et réception) impliquant tous les processus du communicateur (**COMM_WORLD**). Elles pourraient être réalisées par des communications point-à-point mais l'implémentation de ces fonctions par MPI sont optimisées et peuvent tenir compte de l'architecture de la machine (comment les processeurs sont organisées par rapport à la mémoire etc).\n",
    "\n",
    "### 2.1 L'ensemble\n",
    "\n",
    "Ces fonctions sont les suivantes et leur utilisatoin est détaillée vu dans les sections suivantes\n",
    "- une barrière de synchronisation : définir un point dans le programme qui permet d'attendre que tous les processus soient arrivés à ce niveau des instructions.\n",
    "- les communications diffusion - découpage - rassemblement : un seul appel de la fonction pour réaliser l'ensemblee des communications envoi/réception pour transmettre ou rassembler des données. \n",
    "- la réduction\n",
    "\n",
    "**ATTENTION : Une fonction de communications collectives doit être appelée par tous les processus du communicateur impliqué.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 La barrière de synchronisation\n",
    "Les processus exécutent leur programme de manière asynchrone. Cette fonction permet dans un programme d'indiquer un endroit où tous les processus vont s'attendre. L'instruction qui suit cette barrière ne sera exécutée que si tous les processus ont terminé les instructions qui précèdent la barrière.\n",
    "\n",
    "Le coût d'une barrière est vraiment important. **Il faut limiter au minimum l'utilisation de cette barrière.**\n",
    "\n",
    "**Exercice 1:** Testez le programme suivant en observant l'ordre des affichages. Puis rajoutez l'instruction *comm.Barrier()* entre les 2 instructions *print*. Quelle est la différence ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting barrier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile barrier.py\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"Hello de \", rank, \" sur \", size, \" processus dans COMM_WORLD\")\n",
    "comm.Barrier()\n",
    "print(\"Goodbye de \", rank, \" sur \", size, \" processus dans COMM_WORLD\")\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello de  3  sur  4  processus dans COMM_WORLD\n",
      "Goodbye de  3  sur  4  processus dans COMM_WORLD\n",
      "2.1219253540039062e-05\n",
      "Hello de  0  sur  4  processus dans COMM_WORLD\n",
      "Goodbye de  0  sur  4  processus dans COMM_WORLD\n",
      "1.7642974853515625e-05\n",
      "Hello de  1  sur  4  processus dans COMM_WORLD\n",
      "Goodbye de  1  sur  4  processus dans COMM_WORLD\n",
      "Hello de  2  sur  4  processus dans COMM_WORLD\n",
      "Goodbye de  2  sur  4  processus dans COMM_WORLD\n",
      "2.288818359375e-05\n",
      "2.288818359375e-05\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 barrier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 La diffusion\n",
    "\n",
    "Une diffusion est une communication de **un vers tous**. Il s'agit de transmettre une donnée d'un processus à tous les autres processus du communicateur.\n",
    "\n",
    "Là encore **mpi4py** fait la distinction des fonctions du type des données. \n",
    "\n",
    "- bcast pour \"any object python\"\n",
    "- Bcast pour des \"memory buffers\"\n",
    "\n",
    "Pour ces communications, un processus va jouer un rôle différent. Ce processus est appelé processus ***root***. Ce processus est souvent celui de $rank=0$ mais ce n'est pas une obligation. Un programme MPI doit fonctionner quelque soit le *rank* de ce processus et en général le *rank* de ***root*** est donné en argument du programme.\n",
    "\n",
    "Soit l'exemple ci-dessous qui réalise une diffusion d'un scalaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exemple_bcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_bcast.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "root = int(sys.argv[1])\n",
    "\n",
    "data = rank\n",
    "\n",
    "print(\"je suis\", rank, \" data avant =\", data)\n",
    "\n",
    "data = comm.bcast(data, root)\n",
    "\n",
    "print(\"je suis\", rank, \" data après =\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis 0  data avant = 0\n",
      "je suis 2  data avant = 2\n",
      "je suis 2  data après = 0\n",
      "je suis 3  data avant = 3\n",
      "je suis 0  data après = 0\n",
      "je suis 1  data avant = 1\n",
      "je suis 1  data après = 0\n",
      "je suis 3  data après = 0\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_bcast.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'instruction *data = comm.bcast(data, root)* est exécutée par tous les processus. Le sens de *data* est différent en fonction du *rank* des processus. *data* comme paramètre de la fonction est la donnée que le processus *root* souhaite diffuser et *data* comme valeur de retour est la donnée reçue par tous. \n",
    "\n",
    "**Exercice 2:** Modifiez le code ci-dessus pour que la valeur reçue ne soit pas reçue dans data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_bcast_bis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_bcast_bis.py\n",
    "#Nouvelle version\n",
    "\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "root = int(sys.argv[1])\n",
    "\n",
    "data = rank\n",
    "\n",
    "print(\"je suis\", rank, \" data avant =\", data)\n",
    "\n",
    "datarecue = comm.bcast(data, root)\n",
    "\n",
    "print(\"je suis\", rank, \" data après =\", datarecue, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis 3  data avant = 3\n",
      "je suis 1  data avant = 1\n",
      "je suis 0  data avant = 0\n",
      "je suis 2  data avant = 2\n",
      "je suis 1  data après = 0\n",
      "je suis 3  data après = 0\n",
      "je suis 0  data après = 0\n",
      "je suis 2  data après = 0\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_bcast_bis.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple2_bcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple2_bcast.py\n",
    "from mpi4py import MPI\n",
    "import random\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "root = int(sys.argv[1])\n",
    "\n",
    "if rank == root:\n",
    "    tab = [random.randint(1,50) for _ in range(10)]\n",
    "    print(\"tab sur \",rank, \" : \", tab)\n",
    "else:\n",
    "    tab = None\n",
    "\n",
    "tab = comm.bcast(tab, root)\n",
    "\n",
    "print(\"je suis\", rank, \" tab après =\", tab, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab sur  1  :  [2, 15, 19, 28, 35, 1, 23, 3, 2, 2]\n",
      "je suis 1  tab après = [2, 15, 19, 28, 35, 1, 23, 3, 2, 2]\n",
      "je suis 2  tab après = [2, 15, 19, 28, 35, 1, 23, 3, 2, 2]\n",
      "je suis 3  tab après = [2, 15, 19, 28, 35, 1, 23, 3, 2, 2]\n",
      "je suis 0  tab après = [2, 15, 19, 28, 35, 1, 23, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple2_bcast.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple 3 lorsque les données sont des memory buffers de la bibliothèque numpy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple3_Bcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple3_Bcast.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "root = int(sys.argv[1])\n",
    "\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, 10, dtype='i')\n",
    "    print(\"data sur \", rank, \" : \", data)\n",
    "else:\n",
    "    data = np.empty(10, dtype='i')\n",
    "\n",
    "comm.Bcast([data, 10, MPI.INT], root)\n",
    "\n",
    "print(\"je suis\", rank, \" data après =\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n",
      "data sur  0  :  [43  2  3  9  6 33 31 34 20 17]\n",
      "je suis 0  data après = [43  2  3  9  6 33 31 34 20 17]\n",
      "je suis 2  data après = [43  2  3  9  6 33 31 34 20 17]\n",
      "je suis 1  data après = [43  2  3  9  6 33 31 34 20 17]\n",
      "je suis 3  data après = [43  2  3  9  6 33 31 34 20 17]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple3_Bcast.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 La distribution et le rassemblement\n",
    "\n",
    "Lorsqu'on manipule des ensembles de données, ces données sont stockées dans des *tableaux*. En programmation ces tableaux peuvent prendre la forme de différentes structures de données (liste, tableau à 2 dimensions, ...) mais ici on travaillera essentiellement sur des tableaux à une dimension.\n",
    "\n",
    "On a vu que la parallélisation sur des architectures à mémoire distribuée s'appuiait sur la distribution des données afin de répartir les calculs à effectuer. De plus initialement les données sont stockées sur un processeur particulier. Il est donc nécessaire de pouvoir découper et transmettre ces données.\n",
    "\n",
    "Les fonctions disponibles sont les suivantes\n",
    "\n",
    "Lorsque la taille est divisible par le nombre de processus\n",
    "- scatter/Scatter : pour le découpage d'un tableau (un vers tous)\n",
    "- gather/Gather (tous vers un)\n",
    "  \n",
    "Lorsque la taille n'est pas divisible par le nombre de processus\n",
    "- Scatterv\n",
    "- Gatherv\n",
    "\n",
    "Exemple 1 à exécuter sur 4 processus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_scatter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_scatter.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "root = int(sys.argv[1])\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = [[0,1,2,3], [4,5,6,7], [8,9,10,11], [12,13,14,15]]\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "data_local = comm.scatter(data, root)\n",
    "\n",
    "print(\"je suis\", rank, \" data_local =\", data_local, flush = True)\n",
    "\n",
    "data_check = None\n",
    "\n",
    "data_check = comm.gather(data_local, root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"data rassemblement dans\",rank,\" : \", data_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]\n",
      "je suis 2  data_local = [8, 9, 10, 11]\n",
      "je suis 0  data_local = [0, 1, 2, 3]\n",
      "je suis 1  data_local = [4, 5, 6, 7]\n",
      "je suis 3  data_local = [12, 13, 14, 15]\n",
      "data rassemblement dans 0  :  [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_scatter.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce premier exemple, observez la forme des données et comment elles sont réparties. *data* contient 4 tableaux qui sont répartis sur les 4 processus.\n",
    "\n",
    "Exemple 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exemple2_scatter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple2_scatter.py \n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "if length % size != 0:\n",
    "    if rank == root:\n",
    "        print(\" le programme prend 2 arguments\")\n",
    "        print(\" la taille du tableau à découper qui doit être divisible par le nombre de processus\")\n",
    "        print(\" le processus root\")\n",
    "    exit()\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "length_local = length // size\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatter(data, data_local, root)\n",
    "print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_check = np.empty(length, dtype='i')\n",
    "\n",
    "comm.Gather(data_local, data_check, root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"data rassemblement:\", data_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [34 44 34 21 32 13  5 41 35  9 45 49 13 21  1 39 13 37 47  6 39 40 32 29\n",
      " 23  5 40 22 34 18  4 25]\n",
      "je suis 0  data_local = [34 44 34 21 32 13  5 41]\n",
      "je suis 2  data_local = [13 37 47  6 39 40 32 29]\n",
      "je suis 3  data_local = [23  5 40 22 34 18  4 25]\n",
      "je suis 1  data_local = [35  9 45 49 13 21  1 39]\n",
      "data rassemblement: [34 44 34 21 32 13  5 41 35  9 45 49 13 21  1 39 13 37 47  6 39 40 32 29\n",
      " 23  5 40 22 34 18  4 25]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple2_scatter.py 32 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction Scatter travaille sur des buffers numpy et permet de découper ce tableau pour distribuer les morceaux sur chaque processus. Par contre elle suppose que la taille du tableau est divisible par le nombre de processus. Cette hypothèse est aussi faite par la fonction Gather qui rassemble les morceaux.\n",
    "\n",
    "L'exemple suivant lève l'hypothèse de la taille divisible par le nombre de processus. On ne peut travailler alors qu'avec les fonction *Scatterv/Gatherv* et donc des **memory buffers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_scatter_nondivisible.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_scatter_nondivisible.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "data = None\n",
    "\n",
    "length_local = length // size\n",
    "reste = length % size\n",
    "\n",
    "\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "    count = np.empty(size, dtype='i')\n",
    "    for i in range(0, size):\n",
    "        if i < reste:\n",
    "            count[i] = length_local + 1\n",
    "        else:\n",
    "            count[i] = length_local\n",
    "else:\n",
    "    count = None\n",
    "\n",
    "if rank < reste:\n",
    "    length_local += 1\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatterv([data, count, MPI.INT], [data_local, length_local, MPI.INT], root)\n",
    "\n",
    "print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_check = np.empty(length, dtype='i')\n",
    "\n",
    "comm.Gatherv([data_local, length_local, MPI.INT], [data_check, count, MPI.INT], root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"data rassemblement:\", data_check, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [18  6 47 10 14  1 12 32 36 37 28 46  7 39 27 38  1 21 47 42 15 27  6 28\n",
      " 47 11 30  6  7 24 33 24 37 16]\n",
      "je suis 1  data_local = [37 28 46  7 39 27 38  1 21]\n",
      "je suis 2  data_local = [47 42 15 27  6 28 47 11]\n",
      "je suis 0  data_local = [18  6 47 10 14  1 12 32 36]\n",
      "je suis 3  data_local = [30  6  7 24 33 24 37 16]\n",
      "data rassemblement: [18  6 47 10 14  1 12 32 36 37 28 46  7 39 27 38  1 21 47 42 15 27  6 28\n",
      " 47 11 30  6  7 24 33 24 37 16]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_scatter_nondivisible.py 34 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3:** Ecrivez le programme qui permet à un processus *root* de créer un tableau d'entiers et qui calcule le minimum de ce tableau. Le résultat doit être disponible uniquement sur le processus *root*. La parallélisation consiste à partager le tableau afin que chacun calcul un minimum en local et ensuite le calcul est finalisé sur le processus *root* qui doit donc recevoir de la part de chacun le minimum qu'il a calculé. \n",
    "1. faîtes l'hypothèse que le tableau a une taille divisible par le nombre de processus.\n",
    "2. Levez cette hypothèse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercice_minimum.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercice_minimum.py \n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "if length % size != 0:\n",
    "    if rank == root:\n",
    "        print(\" le programme prend 2 arguments\")\n",
    "        print(\" la taille du tableau à découper qui doit être divisible par le nombre de processus\")\n",
    "        print(\" le processus root\")\n",
    "    exit()\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "length_local = length // size\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatter(data, data_local, root)\n",
    "\n",
    "#print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_check = np.empty(size, dtype='i')\n",
    "\n",
    "for i in range(0, size):\n",
    "    minlocal = np.min(data_local)   \n",
    "    \n",
    "    comm.Gather(np.array([minlocal]), data_check, root)\n",
    "    \n",
    "\n",
    "if rank == root:\n",
    "    minglobal = np.min(data_check)\n",
    "    print(\"data rassemblement:\", minglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [24 10 46 30 29 37 25 37 18 41 27 34 31  6 11  8 37  6 24 38]\n",
      "data rassemblement: 6\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exercice_minimum.py 20 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_scatter_nondivisible.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_scatter_nondivisible.py \n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "data = None\n",
    "\n",
    "length_local = length // size\n",
    "reste = length % size\n",
    "\n",
    "\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "    count = np.empty(size, dtype='i')\n",
    "    for i in range(0, size):\n",
    "        if i < reste:\n",
    "            count[i] = length_local + 1\n",
    "        else:\n",
    "            count[i] = length_local\n",
    "else:\n",
    "    count = None\n",
    "\n",
    "if rank < reste:\n",
    "    length_local += 1\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatterv([data, count, MPI.INT], [data_local, length_local, MPI.INT], root)\n",
    "\n",
    "#print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_check = np.empty(size, dtype='i')\n",
    "\n",
    "minlocal = np.min(data_local)   \n",
    "comm.Gather(np.array([minlocal]), data_check, root)\n",
    "    \n",
    "if rank == root:\n",
    "    minglobal = np.min(data_check)\n",
    "    print(\"le minimumm du tableau est:\", minglobal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [25 32 14  6 41 28 46 28  4 49 30 49 18 45 21 28  7 29 46 24  4 18  4 13\n",
      "  1 24 21 32 41 16  7 24 12 32]\n",
      "le minimumm du tableau est: 1\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_scatter_nondivisible.py 34 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4:** Reprendre le programme précédent mais on souhaite maintenant connaître le minimum et la position du minimum dans le tableau. Considérez le cas général où la taille n'est pas divisible par le nombre de processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_scatter_nondivisible_pos.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_scatter_nondivisible_pos.py \n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "data = None\n",
    "\n",
    "length_local = length // size\n",
    "reste = length % size\n",
    "\n",
    "\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "    count = np.empty(size, dtype='i')\n",
    "    for i in range(0, size):\n",
    "        if i < reste:\n",
    "            count[i] = length_local + 1\n",
    "        else:\n",
    "            count[i] = length_local\n",
    "else:\n",
    "    count = None\n",
    "\n",
    "if rank < reste:\n",
    "    length_local += 1\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatterv([data, count, MPI.INT], [data_local, length_local, MPI.INT], root)\n",
    "\n",
    "#print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_check = np.empty(size, dtype='i')\n",
    "\n",
    "minlocal = np.min(data_local)   \n",
    "comm.Gather(np.array([minlocal]), data_check, root)\n",
    "    \n",
    "if rank == root:\n",
    "    minglobal = np.min(data_check)\n",
    "    pos = np.argmin(data)\n",
    "    print(\"le minimumm du tableau est:\", minglobal,' et il se situe à la position', pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [13  3 30  1 46 45 19 17 14 12  1 17 25  9 43 12 37 12  2 34 46 11 24 49\n",
      " 41  4 22 11 38 20 24 11 12 47]\n",
      "le minimumm du tableau est: 1  et il se situe à la position 3\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_scatter_nondivisible_pos.py 34 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 La réduction\n",
    "\n",
    "Lorsqu'à partir d'un ensemble de données on souhaite appliquer une opération **OP** commutative et associative (la somme cumulées de nombre, le minimum d'un ensemble de valeurs, ...) on effectue une réduction à partir de **OP**. Pour la parallélisation, il s'agit de faire un calcul en local et ensuite de propager le résultat tout en appliquant l'opération. \n",
    "\n",
    "MPI implémente la réduction à partir d'un ensemble d'opérateurs disponibles (***MPI.SUM***, ***MPI.MIN***, ...).\n",
    "\n",
    "Exemple 1 avec *reduce* pour travailler sur des *any python objects* : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple_reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "data = None\n",
    "\n",
    "length_local = length // size\n",
    "reste = length % size\n",
    "\n",
    "\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"données initiales : \", data)\n",
    "    count = np.empty(size, dtype='i')\n",
    "    for i in range(0, size):\n",
    "        if i < reste:\n",
    "            count[i] = length_local + 1\n",
    "        else:\n",
    "            count[i] = length_local\n",
    "else:\n",
    "    count = None\n",
    "\n",
    "if rank < reste:\n",
    "    length_local += 1\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatterv([data, count, MPI.INT], [data_local, length_local, MPI.INT], root)\n",
    "\n",
    "s = 0\n",
    "for i in range(0, length_local):\n",
    "    s += data_local[i]\n",
    "\n",
    "res = comm.reduce(s, MPI.SUM, root)\n",
    "\n",
    "if (rank==root):\n",
    "    print(\"je suis \", rank, \" et res=\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n",
      "données initiales :  [ 7 20 26 26  3 28 38 25 21 19  1 31 25 46 32 29 14  9 44  7 22 41  3 19\n",
      " 42  5 41 44 44 49 40 20 25 26]\n",
      "je suis  0  et res= 872\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple_reduce.py 34 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5 :** Explicitez les paramètres de *Scatterv* dans cet exemple pour un tableau de taille 22 et une exécution sur 4 processus.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Votre réponse : comm.Scatterv([data, [6,6,5,5], MPI.INT], [data_local, lenght_local, MPI.INT], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple 2 : Attention on utilise un Scatter donc la taille de données doit être divible par le nombre de processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exemple2_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple2_reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "if length % size != 0:\n",
    "    if rank == root:\n",
    "        print(\" le programme prend 2 arguments\")\n",
    "        print(\" la taille du tableau à découper qui doit être divisible par le nombre de processus\")\n",
    "        print(\" le processus root\")\n",
    "    exit()\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "length_local = length // size\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatter(data, data_local, root)\n",
    "print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "data_reduce = np.zeros(length_local, dtype='i')\n",
    "comm.Reduce([data_local, length_local, MPI.INT], [data_reduce, length_local, MPI.INT], MPI.SUM, root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"je suis \", rank, \"data_reduce=\", data_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n",
      "data pour répartition :  [21 12 16  6 30 10 38 35 39 18 27 38 27 49  7 48 47 10 10 44  1 47  7 15\n",
      " 21 40 26  5 43 22  9 20]\n",
      "je suis 2  data_local = [47 10 10 44  1 47  7 15]\n",
      "je suis 0  data_local = [21 12 16  6 30 10 38 35]\n",
      "je suis 1  data_local = [39 18 27 38 27 49  7 48]\n",
      "je suis 3  data_local = [21 40 26  5 43 22  9 20]\n",
      "je suis  2 data_reduce= [128  80  79  93 101 128  61 118]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple2_reduce.py 32 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6:** Explicitez le rôle des paramètres de *Reduce* et donnez un exemple d'un tableau de taille 16 sur 4 processus. Quel est le calcul réellement effectué ? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Votre réponse : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7:** Ecrivez à nouveau un programme pour calculer le minimun d'un tableau d'entiers en n'utilisant que des communications collectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple3_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exemple3_reduce.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])[47 10 10 44  1 47  7 15]\n",
    "\n",
    "if length % size != 0:\n",
    "    if rank == root:\n",
    "        print(\" le programme prend 2 arguments\")\n",
    "        print(\" la taille du tableau à découper qui doit être divisible par le nombre de processus\")\n",
    "        print(\" le processus root\")\n",
    "    exit()\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "length_local = length // size\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatter(data, data_local, root)\n",
    "print(\"je suis\", rank, \" data_local =\", data_local)\n",
    "\n",
    "min_local = np.min(data_local)\n",
    "\n",
    "data_reduce = np.zeros(length_local, dtype='i')\n",
    "comm.Reduce([data_local, length_local, MPI.INT], [data_reduce, length_local, MPI.INT], MPI.MIN, root)\n",
    "\n",
    "if rank == root:\n",
    "    minglobal = np.min(data_reduce)\n",
    "    print(\"je suis \", rank, \"minTab= \", minglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [36 42  8  6 32 35 11 31 25 27 33 12 27 33  2 10 24 16  9  2 34 20 10 32\n",
      " 23 41  2 44 19 48 10 46]\n",
      "je suis 2  data_local = [24 16  9  2 34 20 10 32]\n",
      "je suis 0  data_local = [36 42  8  6 32 35 11 31]\n",
      "je suis 1  data_local = [25 27 33 12 27 33  2 10]\n",
      "je suis 3  data_local = [23 41  2 44 19 48 10 46]\n",
      "je suis  2 minTab=  2\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple3_reduce.py 32 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 8:** Reprenez le programme précédent pour également calculer la position du minimum. Vous pourrez utiliser l'opérateur ***MPI.MINLOC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exemple4_reduce.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile exemple4_reduce.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "length = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "if length % size != 0:\n",
    "    if rank == root:\n",
    "        print(\" le programme prend 2 arguments\")\n",
    "        print(\" la taille du tableau à découper qui doit être divisible par le nombre de processus\")\n",
    "        print(\" le processus root\")\n",
    "    exit()\n",
    "\n",
    "data = None\n",
    "if rank == root:\n",
    "    data = np.random.randint(1, 50, length, dtype='i')\n",
    "    print(\"data pour répartition : \", data)\n",
    "\n",
    "length_local = length // size\n",
    "\n",
    "data_local = np.empty(length_local, dtype='i')\n",
    "\n",
    "comm.Scatter(data, data_local, root)\n",
    "print(\"je sui\", rank, \" data_local =\", data_local)\n",
    "\n",
    "min_local = np.min(data_local)\n",
    "pos = np.argmin(data_local)\n",
    "\n",
    "mini = comm.reduce([min_local,pos + rank*length_local], MPI.MINLOC, root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"Le minimum global se trouve dans le processus \", rank,\", sa valeur est \", mini[0],\" et se trouve à la position \", mini[1],\" du tableau.\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pour répartition :  [29 15 36 46 17 18 11 17 42 21 36 45 30 11 13 11 33 32 25  5]\n",
      "je sui 1  data_local = [18 11 17 42 21]\n",
      "je sui 3  data_local = [11 33 32 25  5]\n",
      "je sui 0  data_local = [29 15 36 46 17]\n",
      "je sui 2  data_local = [36 45 30 11 13]\n",
      "Le minimum global se trouve dans le processus  1 , sa valeur est  5  et se trouve à la position  19  du tableau.\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 exemple4_reduce.py 20 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Précision sur le type des données entre numpy et mpi4py**\n",
    "Dans les exemples, le type des données est précisé à la création des tableaux *numpy* et dans les fonctions de communication. Cette précision n'est pas nécessaire si on travaille sur des données de type simple et si les données sont contiguës en mémoire. Dans ce cas **mpi4py** est capable de déduire le type de données. Sinon voici quelques correspondances entre les types *MPI* et les types *numpy*\n",
    "\n",
    "| `numpy` dtype        | `mpi4py.MPI` type |\n",
    "|----------------------|------------------|\n",
    "| `np.int32`           | `MPI.INT`        |\n",
    "| `np.int64`           | `MPI.LONG`       |\n",
    "| `np.float32`         | `MPI.FLOAT`      |\n",
    "| `np.float64`         | `MPI.DOUBLE`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 9: Norme de Frobenius d'une matrice**\n",
    "On souhaite calculer la norme de Frobenius d'une matrice $M$ de taille $n \\times m$ initialement sur le processus $rank=root$. \n",
    "$$\\left\\| M \\right\\|_F = \\sum_{i,j} m_{i,j}^2$$\n",
    "\n",
    "Soit le code ci-dessous qui permet de générer une matrice sur le processus $rank=root$. Complétez le code afin de\n",
    "1. répartir la matrice par ligne sur les processus\n",
    "2. effectuer le calcul de la somme partielle des éléments reçus\n",
    "3. calculer le résultat final de la norme de Frobenius par une réduction. Ce résultat doit être disponible sur *root*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frobenius.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frobenius.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "n = int(sys.argv[1])\n",
    "m = int(sys.argv[2])\n",
    "root = int(sys.argv[3])\n",
    "\n",
    "M = None\n",
    "n_local = n // size\n",
    "\n",
    "if rank == root:\n",
    "    M = np.random.uniform(low=1.0, high=5.0, size=(n, m))\n",
    "    #M = np.full((n,m),1.0) # autre version pour les tests\n",
    "\n",
    "M_local = np.empty((n_local,m), dtype='float64')\n",
    "\n",
    "# à compléter ici \n",
    "comm.Scatter(M , M_local ,root = root)\n",
    "\n",
    "\n",
    "res = np.sum(M_local**2)\n",
    "print(res)\n",
    "\n",
    "liste_carree = []\n",
    "for i in range(n_local):\n",
    "    carree =0\n",
    "    for j in range(len(M_local[i])):\n",
    "        carree += M_local[i][j]**2\n",
    "    liste_carree.append(carree)\n",
    "\n",
    "sommepartielle = np.sum(np.array(liste_carree))\n",
    "        \n",
    "res = comm.reduce(sommepartielle, MPI.SUM, root)\n",
    "\n",
    "if rank == root:\n",
    "    print(\"je suis le processus \",rank,\" et la norme de Frobenus est \", res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593.9601907148932\n",
      "660.2096555712837\n",
      "606.8791540621814\n",
      "664.3903562781653\n",
      "je suis le processus  0  et la norme de Frobenus est  2525.4393566265235\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python3 frobenius.py 16 16 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 10: Suite croissante**\n",
    "On souhaite définir si une suite d'entiers est une suite croissante. Complétez le programme ci-dessous pour que les calculs nécessaires à cette vérification soient réparties sur les processus d'une exécution parallèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile suite.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "## tester si une suite est croissante avec les 2 fonctions numpy diff et all.\n",
    "def est_croissante(s):\n",
    "    return np.all(np.diff(s) > 0)\n",
    "    \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "n = int(sys.argv[1])\n",
    "root = int(sys.argv[2])\n",
    "\n",
    "S = None\n",
    "\n",
    "if (rank==root):\n",
    "    S = np.linspace(0.0,10.0, n) # construction d'une suite croissante.\n",
    "    # pour les tests modifier des valeurs pour la rendre non croissante\n",
    "S_local = np.empty(n//size)\n",
    "\n",
    "comm.Scatter(S, S_local, root)\n",
    "\n",
    "# à compléter ici.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 ./suite.py 16 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
